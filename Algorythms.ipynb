   "source": [
    "# This a python code for data mining term project: Song Recommendation Project [ref: [9]]\n",
    "# Shima AzizzadehRoodpish\n",
    "# Spring 2020\n",
    "# No Copyright\n",
    "\n",
    "# Calling internal libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "import pylab as pl\n",
    "\n",
    "# Calling external libraries\n",
    "import Recommenders as Recommenders\n",
    "import Evaluation as Evaluation\n",
    "\n",
    "\n",
    "# Reading input files\n",
    "song_df_1 = pd.read_fwf(r'history.txt') #Reading triplets text file as fixed column width\n",
    "song_df_1.columns = [\"user_id\",\"song_id\",\"listen_count\"] #Adding Headers\n",
    "song_df_2 = pd.read_csv(r'song_data.csv') #Reading .cvs file\n",
    " \n",
    "    \n",
    "# Merging files, dropping duplicates    \n",
    "song_df = pd.merge(song_df_1, song_df_2.drop_duplicates(['song_id']), on=\"song_id\", how=\"left\")    \n",
    "song_df.to_csv(r'millionsong.csv', index = False)\n",
    "\n",
    "# Getting partial output\n",
    "song_df.head()   \n",
    "len(song_df)\n",
    "\n",
    "song_df = song_df.head(10000)\n",
    "#Merge song title and artist_name columns to make a merged column\n",
    "song_df['song_id'] = song_df['title'].map(str) + \" - \" + song_df['artist_name']\n",
    "\n",
    "# The song and artist name are merged into one column, aggregated by number of repeats in general by all users. The code group the song by number of repeats ascending. It calculates the group sum next. Then it calculates this percentage by dividing the repeats by the total sum and then multiply by 100. The ascending order of will goes from most popularity to less popularity.\n",
    "song_grouped = song_df.groupby(['song_id']).agg({'listen_count': 'count'}).reset_index()\n",
    "grouped_sum = song_grouped['listen_count'].sum()\n",
    "song_grouped['percentage']  = song_grouped['listen_count'].div(grouped_sum)*100\n",
    "song_grouped.sort_values(['listen_count', 'song_id'], ascending = [0,1])\n",
    "\n",
    "# Getting the length of uique users and unique songs\n",
    "users = song_df['user_id'].unique()\n",
    "len(users)\n",
    "songs = song_df['song_id'].unique()\n",
    "len(songs)\n",
    "\n",
    "# Dividing data to 80% trainng and 20% testing.\n",
    "train_data, test_data = train_test_split(song_df, test_size = 0.20, random_state=0)\n",
    "# #Uncomment for print\n",
    "# print(train_data.head(5))\n",
    "pm = Recommenders.popularity_recommender_py()\n",
    "pm.create(train_data, 'user_id', 'song_id')\n",
    "\n",
    "user_id = users[5]\n",
    "pm.recommend(user_id)\n",
    "\n",
    "# The recommender class [9] is used for training the model\n",
    "is_model = Recommenders.item_similarity_recommender_py()\n",
    "is_model.create(train_data, 'user_id', 'song_id')\n",
    "\n",
    "# #Printing sample results for popularity based method\n",
    "# user_id = users[5]\n",
    "# user_items = is_model.get_user_items(user_id)\n",
    "## Uncomment for print\n",
    "# print(\"Training data songs suggested for the user:\")\n",
    "# print(user_id)\n",
    "# print(\"------------------------------------------------------------------------------------\")\n",
    "# for user_item in user_items:\n",
    "#     print(user_item)\n",
    "# print(\"...\")\n",
    "\n",
    "\n",
    "\n",
    "# Building a song recommender with personalization (item based) \n",
    "is_model = Recommenders.item_similarity_recommender_py()\n",
    "is_model.create(train_data, 'user_id', 'song_id')\n",
    "\n",
    "# #Print the songs for the user in training data (uncomment if print required)\n",
    "# user_id = users[5]\n",
    "# user_items = is_model.get_user_items(user_id)\n",
    "# #\n",
    "# print(\"Training data songs for the user userid:\")\n",
    "# print(user_id)\n",
    "# print(\"------------------------------------------------------------------------------------\")\n",
    "\n",
    "# for user_item in user_items:\n",
    "#     print(user_item)\n",
    "# print(\"...\")\n",
    "\n",
    "# #Recommend songs for the user using personalized model\n",
    "# is_model.recommend(user_id)\n",
    "\n",
    "#Getting similarity by song id\n",
    "is_model.get_similar_items(['EMOTIONS - Mariah Carey'])\n",
    "\n",
    "\n",
    "\n",
    "#Evaluation process for comparing two methods\n",
    "start = time.time()\n",
    "\n",
    "#Define what percentage of users to use for precision recall calculation\n",
    "user_sample = 0.05\n",
    "\n",
    "\n",
    "#Instantiate the precision_recall_calculator class\n",
    "pr = Evaluation.precision_recall_calculator(test_data, train_data, pm, is_model)\n",
    "\n",
    "\n",
    "#Call method to calculate precision and recall values\n",
    "# (pm_avg_precision_list, pm_avg_recall_list, ism_avg_precision_list, ism_avg_recall_list) = pr.calculate_measures(user_sample)\n",
    "\n",
    "# print(\"test\")                       \n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Plotting precision recall curves.\")\n",
    "\n",
    "# plot_precision_recall(pm_avg_precision_list, pm_avg_recall_list, \"popularity_model\",\n",
    "#                       ism_avg_precision_list, ism_avg_recall_list, \"item_similarity_model\")\n",
    "\n",
    "\n",
    "# #Method to generate precision and recall curve\n",
    "# def plot_precision_recall(m1_precision_list, m1_recall_list, m1_label, m2_precision_list, m2_recall_list, m2_label):\n",
    "#     pl.clf()    \n",
    "#     pl.plot(m1_recall_list, m1_precision_list, label=m1_label)\n",
    "#     pl.plot(m2_recall_list, m2_precision_list, label=m2_label)\n",
    "#     pl.xlabel('Recall')\n",
    "#     pl.ylabel('Precision')\n",
    "#     pl.ylim([0.0, 0.20])\n",
    "#     pl.xlim([0.0, 0.20])\n",
    "#     pl.title('Precision-Recall curve')\n",
    "#     #pl.legend(loc=\"upper right\")\n",
    "#     pl.legend(loc=9, bbox_to_anchor=(0.5, -0.2))\n",
    "#     pl.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Class for Popularity based Recommender System model\n",
    "class popularity_recommender_py():\n",
    "    def __init__(self):\n",
    "        self.train_data = None\n",
    "        self.user_id = None\n",
    "        self.item_id = None\n",
    "        self.popularity_recommendations = None\n",
    "        \n",
    "    #Create the popularity based recommender system model\n",
    "    def create(self, train_data, user_id, item_id):\n",
    "        self.train_data = train_data\n",
    "        self.user_id = user_id\n",
    "        self.item_id = item_id\n",
    "\n",
    "        #Get a count of user_ids for each unique song as recommendation score\n",
    "        train_data_grouped = train_data.groupby([self.item_id]).agg({self.user_id: 'count'}).reset_index()\n",
    "        train_data_grouped.rename(columns = {'user_id': 'score'},inplace=True)\n",
    "    \n",
    "        #Sort the songs based upon recommendation score\n",
    "        train_data_sort = train_data_grouped.sort_values(['score', self.item_id], ascending = [0,1])\n",
    "    \n",
    "        #Generate a recommendation rank based upon score\n",
    "        train_data_sort['Rank'] = train_data_sort['score'].rank(ascending=0, method='first')\n",
    "        \n",
    "        #Get the top 10 recommendations\n",
    "        self.popularity_recommendations = train_data_sort.head(10)\n",
    "\n",
    "    #Use the popularity based recommender system model to\n",
    "    #make recommendations\n",
    "    def recommend(self, user_id):    \n",
    "        user_recommendations = self.popularity_recommendations\n",
    "        \n",
    "        #Add user_id column for which the recommendations are being generated\n",
    "        user_recommendations['user_id'] = user_id\n",
    "    \n",
    "        #Bring user_id column to the front\n",
    "        cols = user_recommendations.columns.tolist()\n",
    "        cols = cols[-1:] + cols[:-1]\n",
    "        user_recommendations = user_recommendations[cols]\n",
    "        \n",
    "        return user_recommendations\n",
    "    \n",
    "\n",
    "#Class for Item similarity based Recommender System model\n",
    "class item_similarity_recommender_py():\n",
    "    def __init__(self):\n",
    "        self.train_data = None\n",
    "        self.user_id = None\n",
    "        self.item_id = None\n",
    "        self.cooccurence_matrix = None\n",
    "        self.songs_dict = None\n",
    "        self.rev_songs_dict = None\n",
    "        self.item_similarity_recommendations = None\n",
    "        \n",
    "    #Get unique items (songs) corresponding to a given user\n",
    "    def get_user_items(self, user):\n",
    "        user_data = self.train_data[self.train_data[self.user_id] == user]\n",
    "        user_items = list(user_data[self.item_id].unique())\n",
    "        \n",
    "        return user_items\n",
    "        \n",
    "    #Get unique users for a given item (song)\n",
    "    def get_item_users(self, item):\n",
    "        item_data = self.train_data[self.train_data[self.item_id] == item]\n",
    "        item_users = set(item_data[self.user_id].unique())\n",
    "            \n",
    "        return item_users\n",
    "        \n",
    "    #Get unique items (songs) in the training data\n",
    "    def get_all_items_train_data(self):\n",
    "        all_items = list(self.train_data[self.item_id].unique())\n",
    "            \n",
    "        return all_items\n",
    "        \n",
    "    #Construct cooccurence matrix\n",
    "    def construct_cooccurence_matrix(self, user_songs, all_songs):\n",
    "            \n",
    "        ####################################\n",
    "        #Get users for all songs in user_songs.\n",
    "        ####################################\n",
    "        user_songs_users = []        \n",
    "        for i in range(0, len(user_songs)):\n",
    "            user_songs_users.append(self.get_item_users(user_songs[i]))\n",
    "            \n",
    "        ###############################################\n",
    "        #Initialize the item cooccurence matrix of size \n",
    "        #len(user_songs) X len(songs)\n",
    "        ###############################################\n",
    "        cooccurence_matrix = np.matrix(np.zeros(shape=(len(user_songs), len(all_songs))), float)\n",
    "           \n",
    "        #############################################################\n",
    "        #Calculate similarity between user songs and all unique songs\n",
    "        #in the training data\n",
    "        #############################################################\n",
    "        for i in range(0,len(all_songs)):\n",
    "            #Calculate unique listeners (users) of song (item) i\n",
    "            songs_i_data = self.train_data[self.train_data[self.item_id] == all_songs[i]]\n",
    "            users_i = set(songs_i_data[self.user_id].unique())\n",
    "            \n",
    "            for j in range(0,len(user_songs)):       \n",
    "                    \n",
    "                #Get unique listeners (users) of song (item) j\n",
    "                users_j = user_songs_users[j]\n",
    "                    \n",
    "                #Calculate intersection of listeners of songs i and j\n",
    "                users_intersection = users_i.intersection(users_j)\n",
    "                \n",
    "                #Calculate cooccurence_matrix[i,j] as Jaccard Index\n",
    "                if len(users_intersection) != 0:\n",
    "                    #Calculate union of listeners of songs i and j\n",
    "                    users_union = users_i.union(users_j)\n",
    "                    \n",
    "                    cooccurence_matrix[j,i] = float(len(users_intersection))/float(len(users_union))\n",
    "                else:\n",
    "                    cooccurence_matrix[j,i] = 0\n",
    "                    \n",
    "        \n",
    "        return cooccurence_matrix\n",
    "\n",
    "    \n",
    "    #Use the cooccurence matrix to make top recommendations\n",
    "    def generate_top_recommendations(self, user, cooccurence_matrix, all_songs, user_songs):\n",
    "        print(\"Non zero values in cooccurence_matrix :%d\" % np.count_nonzero(cooccurence_matrix))\n",
    "        \n",
    "        #Calculate a weighted average of the scores in cooccurence matrix for all user songs.\n",
    "        user_sim_scores = cooccurence_matrix.sum(axis=0)/float(cooccurence_matrix.shape[0])\n",
    "        user_sim_scores = np.array(user_sim_scores)[0].tolist()\n",
    " \n",
    "        #Sort the indices of user_sim_scores based upon their value\n",
    "        #Also maintain the corresponding score\n",
    "        sort_index = sorted(((e,i) for i,e in enumerate(list(user_sim_scores))), reverse=True)\n",
    "    \n",
    "        #Create a dataframe from the following\n",
    "        columns = ['user_id', 'song_id', 'score', 'rank']\n",
    "        #index = np.arange(1) # array of numbers for the number of samples\n",
    "        df = pandas.DataFrame(columns=columns)\n",
    "         \n",
    "        #Fill the dataframe with top 10 item based recommendations\n",
    "        rank = 1 \n",
    "        for i in range(0,len(sort_index)):\n",
    "            if ~np.isnan(sort_index[i][0]) and all_songs[sort_index[i][1]] not in user_songs and rank <= 10:\n",
    "                df.loc[len(df)]=[user,all_songs[sort_index[i][1]],sort_index[i][0],rank]\n",
    "                rank = rank+1\n",
    "        \n",
    "        #Handle the case where there are no recommendations\n",
    "        if df.shape[0] == 0:\n",
    "            print(\"The current user has no songs for training the item similarity based recommendation model.\")\n",
    "            return -1\n",
    "        else:\n",
    "            return df\n",
    " \n",
    "    #Create the item similarity based recommender system model\n",
    "    def create(self, train_data, user_id, item_id):\n",
    "        self.train_data = train_data\n",
    "        self.user_id = user_id\n",
    "        self.item_id = item_id\n",
    "\n",
    "    #Use the item similarity based recommender system model to\n",
    "    #make recommendations\n",
    "    def recommend(self, user):\n",
    "        \n",
    "        ########################################\n",
    "        #A. Get all unique songs for this user\n",
    "        ########################################\n",
    "        user_songs = self.get_user_items(user)    \n",
    "            \n",
    "        print(\"No. of unique songs for the user: %d\" % len(user_songs))\n",
    "        \n",
    "        ######################################################\n",
    "        #B. Get all unique items (songs) in the training data\n",
    "        ######################################################\n",
    "        all_songs = self.get_all_items_train_data()\n",
    "        \n",
    "        print(\"no. of unique songs in the training set: %d\" % len(all_songs))\n",
    "         \n",
    "        ###############################################\n",
    "        #C. Construct item cooccurence matrix of size \n",
    "        #len(user_songs) X len(songs)\n",
    "        ###############################################\n",
    "        cooccurence_matrix = self.construct_cooccurence_matrix(user_songs, all_songs)\n",
    "        \n",
    "        #######################################################\n",
    "        #D. Use the cooccurence matrix to make recommendations\n",
    "        #######################################################\n",
    "        df_recommendations = self.generate_top_recommendations(user, cooccurence_matrix, all_songs, user_songs)\n",
    "                \n",
    "        return df_recommendations\n",
    "    \n",
    "    #Get similar items to given items\n",
    "    def get_similar_items(self, item_list):\n",
    "        \n",
    "        user_songs = item_list\n",
    "        \n",
    "        ######################################################\n",
    "        #B. Get all unique items (songs) in the training data\n",
    "        ######################################################\n",
    "        all_songs = self.get_all_items_train_data()\n",
    "        \n",
    "        print(\"no. of unique songs in the training set: %d\" % len(all_songs))\n",
    "         \n",
    "        ###############################################\n",
    "        #C. Construct item cooccurence matrix of size \n",
    "        #len(user_songs) X len(songs)\n",
    "        ###############################################\n",
    "        cooccurence_matrix = self.construct_cooccurence_matrix(user_songs, all_songs)\n",
    "        \n",
    "        #######################################################\n",
    "        #D. Use the cooccurence matrix to make recommendations\n",
    "        #######################################################\n",
    "        user = \"\"\n",
    "        df_recommendations = self.generate_top_recommendations(user, cooccurence_matrix, all_songs, user_songs)\n",
    "         \n",
    "        return df_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "\n",
    "#Method to generate precision and recall curve\n",
    "def plot_precision_recall(m1_precision_list, m1_recall_list, m1_label, m2_precision_list, m2_recall_list, m2_label):\n",
    "    pl.clf()    \n",
    "    pl.plot(m1_recall_list, m1_precision_list, label=m1_label)\n",
    "    pl.plot(m2_recall_list, m2_precision_list, label=m2_label)\n",
    "    pl.xlabel('Recall')\n",
    "    pl.ylabel('Precision')\n",
    "    pl.ylim([0.0, 0.20])\n",
    "    pl.xlim([0.0, 0.20])\n",
    "    pl.title('Precision-Recall curve')\n",
    "    #pl.legend(loc=\"upper right\")\n",
    "    pl.legend(loc=9, bbox_to_anchor=(0.5, -0.2))\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
